{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled37.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNQKxc4F0x0wiSqJI1nHRk6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/d-maniatakos/vaccine-sentiment-classifier/blob/master/vaccine_sentiment_classifier_using_neural_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MGgWWk4-Sws"
      },
      "source": [
        "## **Modules Import**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h0TYkVY3ieJ",
        "outputId": "2b71be46-41b2-4e70-9b45-2555ea08794b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchtext\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Download Glove**"
      ],
      "metadata": {
        "id": "GN3eIf4QVcN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBEsnwOlO88s",
        "outputId": "8f2e85ae-6143-4055-c3dd-85963f803809"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-08 13:42:55--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-12-08 13:42:55--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-12-08 13:42:55--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  4.93MB/s    in 2m 57s  \n",
            "\n",
            "2021-12-08 13:45:53 (4.64 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEMVlOG5Or2V",
        "outputId": "3a6e40c6-d513-4837-eddf-46573257e7b2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glove = {}\n",
        "\n",
        "with open('glove.6B.50d.txt') as f:\n",
        "    lines = f.readlines()\n",
        "for line in lines:\n",
        "  vector = []\n",
        "  for dim in line.split(' ')[1:]:\n",
        "    vector.append(float(dim))\n",
        "  glove[line.split(' ')[0]] = np.array(vector)"
      ],
      "metadata": {
        "id": "DKvZQOXfO54w"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MqaN0Qe-aLq"
      },
      "source": [
        "## **Data Import**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jvvl_FgODooA",
        "outputId": "0f68e49b-d35c-4aff-939b-8d89df7474c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "# read datasets and ignore the first (index) column\n",
        "train_data =  pd.read_csv('vaccine_train_set.csv').iloc[:, 1:]\n",
        "validation_data = pd.read_csv('vaccine_validation_set.csv').iloc[:, 1:]    # <--- replace with test set path\n",
        "\n",
        "train_data.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sip N Shop Come thru right now #Marjais #Popul...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I don't know about you but My family and I wil...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@MSignorile Immunizations should be mandatory....</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>President Obama spoke in favor of vaccination ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"@myfoxla: Arizona monitoring hundreds for mea...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               tweet  label\n",
              "0  Sip N Shop Come thru right now #Marjais #Popul...      0\n",
              "1  I don't know about you but My family and I wil...      1\n",
              "2  @MSignorile Immunizations should be mandatory....      2\n",
              "3  President Obama spoke in favor of vaccination ...      0\n",
              "4  \"@myfoxla: Arizona monitoring hundreds for mea...      0"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPxtP__p658s"
      },
      "source": [
        "## **Pre-processing & Vectorization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXoWoOumv9cj"
      },
      "source": [
        "# preprocess tweets texts before vectorization\n",
        "def preprocess(text):\n",
        "  text = text.lower()\n",
        "  unwanted_chars = ['@', '#', '!', '(', ')', '*', ':', ',']\n",
        "\n",
        "  for char in unwanted_chars:\n",
        "    text = text.replace(char, '')\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "  lemmatized_text = ''\n",
        "  for word in text.split():\n",
        "    lemmatized_text += lemmatizer.lemmatize(word) + ' '\n",
        "  return lemmatized_text\n",
        "\n",
        "\n",
        "# vectorize a tweet using glove word embedding\n",
        "def glove_vectorize(corpus, representation = 'average'):\n",
        "  vectors = []\n",
        "  for document in corpus:\n",
        "    glove_vectors = []\n",
        "    for token in document.split():\n",
        "      try:\n",
        "        glove_vectors.append(torch.from_numpy(glove[token]))\n",
        "      except:\n",
        "        glove_vectors.append(torch.from_numpy(np.zeros((50))))\n",
        "    if representation == 'average':\n",
        "      vectors.append(torch.stack(glove_vectors, dim=0).sum(dim=0).div(len(document.split())))\n",
        "    elif representation == 'sum':\n",
        "      vectors.append(torch.stack(glove_vectors, dim=0).sum(dim=0))\n",
        "  return torch.stack(vectors, dim=0).to(torch.float32)\n",
        "\n",
        "# create a tf-idf or bow vectorizer (using the training set's tweets)\n",
        "def create_vectorizer(train_corpus, method='tf-idf', max_features=1000, ngram_range=(1, 2)):\n",
        "  if method == 'tf-idf':\n",
        "    vectorizer = TfidfVectorizer(max_features=max_features, ngram_range=ngram_range)\n",
        "  elif method == 'bow':\n",
        "    vectorizer = CountVectorizer(max_features=max_features, ngram_range=ngram_range)\n",
        "  return vectorizer.fit(train_corpus)\n",
        "\n",
        "# vectorize a dataset\n",
        "def vectorize(vectorizer, corpus):\n",
        "  return torch.from_numpy(vectorizer.transform(corpus).toarray()).to(torch.float32)\n",
        "\n",
        "\n",
        "train_data['tweet'] = train_data['tweet'].apply(preprocess)\n",
        "validation_data['tweet'] = validation_data['tweet'].apply(preprocess)\n",
        "\n",
        "train_corpus = train_data['tweet']\n",
        "validation_corpus = validation_data['tweet']\n",
        "\n",
        "# tf-idf vectorization approach\n",
        "tf_idf_vectorizer = create_vectorizer(train_data['tweet'])\n",
        "train_vector = vectorize(tf_idf_vectorizer, train_corpus)\n",
        "validation_vector = vectorize(tf_idf_vectorizer, validation_corpus)\n",
        "\n",
        "# glove vectorization approach\n",
        "# train_vector = glove_vectorize(train_corpus)\n",
        "# validation_vector = glove_vectorize(validation_corpus)\n",
        "\n",
        "x_train = train_vector\n",
        "y_train = torch.tensor(train_data['label'].values, dtype=torch.long)\n",
        "\n",
        "x_validation = validation_vector\n",
        "y_validation = torch.tensor(validation_data['label'].values, dtype=torch.long)"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Creation, Training & Evaluation**"
      ],
      "metadata": {
        "id": "uTs2bhunafr2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdwFr0n65_YU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f7313a3-2ddc-47b3-f2f9-603714660587"
      },
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "  def __init__(self, x_size, num_of_classes, hidden_size):\n",
        "    super(NeuralNetwork, self).__init__()\n",
        "    self.l1 = nn.Linear(x_size, hidden_size)\n",
        "    self.l2 = nn.ReLU()\n",
        "    self.l3 = nn.ReLU()\n",
        "    self.l4 = nn.Linear(hidden_size, num_of_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.l1(x)\n",
        "    out = self.l2(out)\n",
        "    out = self.l3(out)\n",
        "    out = self.l4(out)\n",
        "    return out\n",
        "\n",
        "input_size = 1000\n",
        "hidden_size = 100\n",
        "output_size = 3\n",
        "num_of_epochs = 100\n",
        "batch_size = 50\n",
        "learning_rate = 0.005\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "model = NeuralNetwork(input_size, output_size, hidden_size)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(num_of_epochs):\n",
        "  batch_losses = []\n",
        "\n",
        "  for index, (x, y) in enumerate(train_dataloader):\n",
        "    y_pred = model(x)\n",
        "    loss = loss_function(y_pred, y)\n",
        "    batch_losses.append(loss.item())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  \n",
        "  print('Epoch: ' + str(epoch+1) + '/' + str(num_of_epochs) + ' Loss: ' + str(sum(batch_losses)/len(train_dataloader)))\n",
        "\n",
        "  \n"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/100 Loss: 0.7315863567590714\n",
            "Epoch: 2/100 Loss: 0.6121771919727326\n",
            "Epoch: 3/100 Loss: 0.5555927259922028\n",
            "Epoch: 4/100 Loss: 0.48131192004680634\n",
            "Epoch: 5/100 Loss: 0.3892309994697571\n",
            "Epoch: 6/100 Loss: 0.29420304387807844\n",
            "Epoch: 7/100 Loss: 0.21003246492147445\n",
            "Epoch: 8/100 Loss: 0.14696762196719645\n",
            "Epoch: 9/100 Loss: 0.10388402876257896\n",
            "Epoch: 10/100 Loss: 0.07897730526328087\n",
            "Epoch: 11/100 Loss: 0.06688874853402377\n",
            "Epoch: 12/100 Loss: 0.05500690709799528\n",
            "Epoch: 13/100 Loss: 0.05300597612932324\n",
            "Epoch: 14/100 Loss: 0.047061178939417005\n",
            "Epoch: 15/100 Loss: 0.04574764510430396\n",
            "Epoch: 16/100 Loss: 0.04270966698974371\n",
            "Epoch: 17/100 Loss: 0.041656162206083536\n",
            "Epoch: 18/100 Loss: 0.04115706650167704\n",
            "Epoch: 19/100 Loss: 0.03915246226638556\n",
            "Epoch: 20/100 Loss: 0.04008368455246091\n",
            "Epoch: 21/100 Loss: 0.041263415371999144\n",
            "Epoch: 22/100 Loss: 0.03656896366085857\n",
            "Epoch: 23/100 Loss: 0.03435428424365818\n",
            "Epoch: 24/100 Loss: 0.0364202398378402\n",
            "Epoch: 25/100 Loss: 0.034807802985887974\n",
            "Epoch: 26/100 Loss: 0.034780606057494876\n",
            "Epoch: 27/100 Loss: 0.03166587051190436\n",
            "Epoch: 28/100 Loss: 0.03342149648535997\n",
            "Epoch: 29/100 Loss: 0.03253826735494658\n",
            "Epoch: 30/100 Loss: 0.03194334022467956\n",
            "Epoch: 31/100 Loss: 0.03173805964086205\n",
            "Epoch: 32/100 Loss: 0.029351531548425555\n",
            "Epoch: 33/100 Loss: 0.030190158504992723\n",
            "Epoch: 34/100 Loss: 0.029475005946587772\n",
            "Epoch: 35/100 Loss: 0.028671119495294987\n",
            "Epoch: 36/100 Loss: 0.029683311510365455\n",
            "Epoch: 37/100 Loss: 0.028117722724564375\n",
            "Epoch: 38/100 Loss: 0.028285946258343755\n",
            "Epoch: 39/100 Loss: 0.02735214925929904\n",
            "Epoch: 40/100 Loss: 0.026687024110462518\n",
            "Epoch: 41/100 Loss: 0.026410658923676236\n",
            "Epoch: 42/100 Loss: 0.026523004099377433\n",
            "Epoch: 43/100 Loss: 0.026912653817329556\n",
            "Epoch: 44/100 Loss: 0.02607640021527186\n",
            "Epoch: 45/100 Loss: 0.027900188409024848\n",
            "Epoch: 46/100 Loss: 0.026171164262108505\n",
            "Epoch: 47/100 Loss: 0.025523613787489012\n",
            "Epoch: 48/100 Loss: 0.02358237755810842\n",
            "Epoch: 49/100 Loss: 0.024509287652559577\n",
            "Epoch: 50/100 Loss: 0.024412145806476472\n",
            "Epoch: 51/100 Loss: 0.023779715647920967\n",
            "Epoch: 52/100 Loss: 0.024415535406093113\n",
            "Epoch: 53/100 Loss: 0.02469667119323276\n",
            "Epoch: 54/100 Loss: 0.023012294349842706\n",
            "Epoch: 55/100 Loss: 0.02400117418426089\n",
            "Epoch: 56/100 Loss: 0.02291344359260984\n",
            "Epoch: 57/100 Loss: 0.023402731296722778\n",
            "Epoch: 58/100 Loss: 0.023320010834606365\n",
            "Epoch: 59/100 Loss: 0.021839518733671865\n",
            "Epoch: 60/100 Loss: 0.02202217926888261\n",
            "Epoch: 61/100 Loss: 0.021174135460867546\n",
            "Epoch: 62/100 Loss: 0.022864391865441577\n",
            "Epoch: 63/100 Loss: 0.021740988943492995\n",
            "Epoch: 64/100 Loss: 0.021870931979618036\n",
            "Epoch: 65/100 Loss: 0.020824723698548042\n",
            "Epoch: 66/100 Loss: 0.020948879286646842\n",
            "Epoch: 67/100 Loss: 0.022852195357205347\n",
            "Epoch: 68/100 Loss: 0.02180947722203564\n",
            "Epoch: 69/100 Loss: 0.021068794743390754\n",
            "Epoch: 70/100 Loss: 0.020412458796985448\n",
            "Epoch: 71/100 Loss: 0.019550207632186355\n",
            "Epoch: 72/100 Loss: 0.019786288613919168\n",
            "Epoch: 73/100 Loss: 0.02017059418396093\n",
            "Epoch: 74/100 Loss: 0.019215743613080122\n",
            "Epoch: 75/100 Loss: 0.020311528497724794\n",
            "Epoch: 76/100 Loss: 0.01899888606223976\n",
            "Epoch: 77/100 Loss: 0.01986713297618553\n",
            "Epoch: 78/100 Loss: 0.019474686647532508\n",
            "Epoch: 79/100 Loss: 0.019389385599817614\n",
            "Epoch: 80/100 Loss: 0.019212553541001397\n",
            "Epoch: 81/100 Loss: 0.01960655518976273\n",
            "Epoch: 82/100 Loss: 0.019327188954106532\n",
            "Epoch: 83/100 Loss: 0.018156489809858612\n",
            "Epoch: 84/100 Loss: 0.01878738499688916\n",
            "Epoch: 85/100 Loss: 0.01801086467067944\n",
            "Epoch: 86/100 Loss: 0.01863661090459209\n",
            "Epoch: 87/100 Loss: 0.018673577257315627\n",
            "Epoch: 88/100 Loss: 0.017851482943398878\n",
            "Epoch: 89/100 Loss: 0.01787803067173809\n",
            "Epoch: 90/100 Loss: 0.018556670296791707\n",
            "Epoch: 91/100 Loss: 0.01794817674299702\n",
            "Epoch: 92/100 Loss: 0.01841600365069462\n",
            "Epoch: 93/100 Loss: 0.018220099244412268\n",
            "Epoch: 94/100 Loss: 0.01806399381911615\n",
            "Epoch: 95/100 Loss: 0.017986373573570746\n",
            "Epoch: 96/100 Loss: 0.017495202709600564\n",
            "Epoch: 97/100 Loss: 0.017557674880546985\n",
            "Epoch: 98/100 Loss: 0.01720139890551218\n",
            "Epoch: 99/100 Loss: 0.016744814436446178\n",
            "Epoch: 100/100 Loss: 0.017072256433515575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.forward(validation_vector[0])\n",
        "torch.argmax(result)\n",
        "\n",
        "predictions = []\n",
        "\n",
        "for i in range(validation_vector.size()[0]):\n",
        "  prediction = model.forward(validation_vector[i])\n",
        "  predictions.append(torch.argmax(prediction).item())\n",
        "\n",
        "predictions\n",
        "y_validation.tolist()\n",
        "\n",
        "precision = precision_score(y_validation.tolist(), predictions, average='weighted')\n",
        "\n",
        "precision"
      ],
      "metadata": {
        "id": "Emp_YS-5hyqe",
        "outputId": "c9b9123f-fd73-48ae-935e-500a6a943854",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6755810737654179"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    }
  ]
}