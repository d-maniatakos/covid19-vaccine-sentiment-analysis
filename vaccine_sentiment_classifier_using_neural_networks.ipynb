{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled37.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN788H3R6vFIqBAqaeEZsPo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/d-maniatakos/vaccine-sentiment-classifier/blob/master/vaccine_sentiment_classifier_using_neural_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MGgWWk4-Sws"
      },
      "source": [
        "## **Modules Import**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h0TYkVY3ieJ",
        "outputId": "5bbbeb84-ddcd-45d0-ce76-f13e922c6232",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchtext\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, log_loss, confusion_matrix, ConfusionMatrixDisplay"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Download Glove**"
      ],
      "metadata": {
        "id": "GN3eIf4QVcN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBEsnwOlO88s",
        "outputId": "48659324-152d-4d2a-e29c-e2a09c77ba27"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-08 18:34:30--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-12-08 18:34:30--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-12-08 18:34:30--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.1’\n",
            "\n",
            "glove.6B.zip.1       78%[==============>     ] 647.56M  --.-KB/s    in 13m 31s \n",
            "\n",
            "2021-12-08 18:50:56 (818 KB/s) - Connection closed at byte 679019064. Retrying.\n",
            "\n",
            "--2021-12-08 18:50:57--  (try: 2)  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 206 Partial Content\n",
            "Length: 862182613 (822M), 183163549 (175M) remaining [application/zip]\n",
            "Saving to: ‘glove.6B.zip.1’\n",
            "\n",
            "glove.6B.zip.1      100%[+++++++++++++++====>] 822.24M  4.86MB/s    in 66s     \n",
            "\n",
            "2021-12-08 18:52:03 (2.64 MB/s) - ‘glove.6B.zip.1’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEMVlOG5Or2V",
        "outputId": "fe53c0f3-c636-4ae1-fb16-445c583423b9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  glove.6B.zip\n",
            "replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: glove.6B.50d.txt        \n",
            "replace glove.6B.100d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: yes\n",
            "  inflating: glove.6B.100d.txt       \n",
            "replace glove.6B.200d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: glove.6B.200d.txt       \n",
            "replace glove.6B.300d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glove = {}\n",
        "\n",
        "with open('glove.6B.50d.txt') as f:\n",
        "    lines = f.readlines()\n",
        "for line in lines:\n",
        "  vector = []\n",
        "  for dim in line.split(' ')[1:]:\n",
        "    vector.append(float(dim))\n",
        "  glove[line.split(' ')[0]] = np.array(vector)"
      ],
      "metadata": {
        "id": "DKvZQOXfO54w"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MqaN0Qe-aLq"
      },
      "source": [
        "## **Data Import**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jvvl_FgODooA",
        "outputId": "76f1ded6-b834-4ffb-9010-77b873f06ba1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "# read datasets and ignore the first (index) column\n",
        "train_data =  pd.read_csv('vaccine_train_set.csv').iloc[:, 1:]\n",
        "validation_data = pd.read_csv('vaccine_validation_set.csv').iloc[:, 1:]    # <--- replace with test set path\n",
        "\n",
        "train_data.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sip N Shop Come thru right now #Marjais #Popul...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I don't know about you but My family and I wil...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@MSignorile Immunizations should be mandatory....</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>President Obama spoke in favor of vaccination ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"@myfoxla: Arizona monitoring hundreds for mea...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               tweet  label\n",
              "0  Sip N Shop Come thru right now #Marjais #Popul...      0\n",
              "1  I don't know about you but My family and I wil...      1\n",
              "2  @MSignorile Immunizations should be mandatory....      2\n",
              "3  President Obama spoke in favor of vaccination ...      0\n",
              "4  \"@myfoxla: Arizona monitoring hundreds for mea...      0"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPxtP__p658s"
      },
      "source": [
        "## **Pre-processing & Vectorization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXoWoOumv9cj"
      },
      "source": [
        "# preprocess tweets texts before vectorization\n",
        "def preprocess(text):\n",
        "  text = text.lower()\n",
        "  unwanted_chars = ['@', '#', '!', '(', ')', '*', ':', ',']\n",
        "\n",
        "  for char in unwanted_chars:\n",
        "    text = text.replace(char, '')\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "  lemmatized_text = ''\n",
        "  for word in text.split():\n",
        "    lemmatized_text += lemmatizer.lemmatize(word) + ' '\n",
        "  return lemmatized_text\n",
        "\n",
        "\n",
        "# vectorize a tweet using glove word embedding\n",
        "def glove_vectorize(corpus, representation = 'average'):\n",
        "  vectors = []\n",
        "  for document in corpus:\n",
        "    glove_vectors = []\n",
        "    for token in document.split():\n",
        "      try:\n",
        "        glove_vectors.append(torch.from_numpy(glove[token]))\n",
        "      except:\n",
        "        glove_vectors.append(torch.from_numpy(np.zeros((50))))\n",
        "    if representation == 'average':\n",
        "      vectors.append(torch.stack(glove_vectors, dim=0).sum(dim=0).div(len(document.split())))\n",
        "    elif representation == 'sum':\n",
        "      vectors.append(torch.stack(glove_vectors, dim=0).sum(dim=0))\n",
        "  return torch.stack(vectors, dim=0).to(torch.float32)\n",
        "\n",
        "# create a tf-idf or bow vectorizer (using the training set's tweets)\n",
        "def create_vectorizer(train_corpus, method='tf-idf', max_features=1000, ngram_range=(1, 2)):\n",
        "  if method == 'tf-idf':\n",
        "    vectorizer = TfidfVectorizer(max_features=max_features, ngram_range=ngram_range)\n",
        "  elif method == 'bow':\n",
        "    vectorizer = CountVectorizer(max_features=max_features, ngram_range=ngram_range)\n",
        "  return vectorizer.fit(train_corpus)\n",
        "\n",
        "# vectorize a dataset\n",
        "def vectorize(vectorizer, corpus):\n",
        "  return torch.from_numpy(vectorizer.transform(corpus).toarray()).to(torch.float32)\n",
        "\n",
        "\n",
        "train_data['tweet'] = train_data['tweet'].apply(preprocess)\n",
        "validation_data['tweet'] = validation_data['tweet'].apply(preprocess)\n",
        "\n",
        "train_corpus = train_data['tweet']\n",
        "validation_corpus = validation_data['tweet']\n",
        "\n",
        "# tf-idf vectorization approach\n",
        "tf_idf_vectorizer = create_vectorizer(train_data['tweet'])\n",
        "train_vector = vectorize(tf_idf_vectorizer, train_corpus)\n",
        "validation_vector = vectorize(tf_idf_vectorizer, validation_corpus)\n",
        "\n",
        "# glove vectorization approach\n",
        "# train_vector = glove_vectorize(train_corpus)\n",
        "# validation_vector = glove_vectorize(validation_corpus)\n",
        "\n",
        "x_train = train_vector\n",
        "y_train = torch.tensor(train_data['label'].values, dtype=torch.long)\n",
        "\n",
        "x_validation = validation_vector\n",
        "y_validation = torch.tensor(validation_data['label'].values, dtype=torch.long)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Creation, Training & Evaluation**"
      ],
      "metadata": {
        "id": "uTs2bhunafr2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdwFr0n65_YU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 727
        },
        "outputId": "f93b9d68-e43a-45af-8f88-566f0f2191e4"
      },
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "  def __init__(self, x_size, num_of_classes, hidden_size):\n",
        "    super(NeuralNetwork, self).__init__()\n",
        "    self.l1 = nn.Linear(x_size, hidden_size)\n",
        "    self.l2 = nn.ReLU()\n",
        "    self.l3 = nn.Linear(hidden_size, num_of_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.l1(x)\n",
        "    out = self.l2(out)\n",
        "    out = self.l3(out)\n",
        "    return out\n",
        "\n",
        "input_size = 1000\n",
        "hidden_size = 300\n",
        "output_size = 3\n",
        "num_of_epochs = 50\n",
        "batch_size = 10000\n",
        "learning_rate = 0.001\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "model = NeuralNetwork(input_size, output_size, hidden_size)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(num_of_epochs):\n",
        "  batch_losses = []\n",
        "\n",
        "  for index, (x, y) in enumerate(train_dataloader):\n",
        "    y_pred = model(x)\n",
        "    loss = loss_function(y_pred, y)\n",
        "    batch_losses.append(loss.item())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  predictions = []\n",
        "\n",
        "  for i in range(validation_vector.size()[0]):\n",
        "    prediction = model.forward(validation_vector[i])\n",
        "    predictions.append(prediction.tolist())\n",
        "\n",
        "  preds = []\n",
        "\n",
        "  for i in range(validation_vector.size()[0]):\n",
        "    prediction = model.forward(validation_vector[i])\n",
        "    preds.append(torch.argmax(prediction).item())\n",
        "\n",
        "  predictions\n",
        "  y_validation.tolist()\n",
        "\n",
        "  precision = precision_score(y_validation.tolist(), preds, average='weighted')\n",
        "\n",
        "  print(precision)\n",
        "  \n",
        "\n",
        "  validation_loss = log_loss(y_validation.tolist(), predictions)\n",
        "\n",
        "  \n",
        "  print('Epoch: ' + str(epoch+1) + '/' + str(num_of_epochs) + ' Training Loss: ' + str(sum(batch_losses)/len(train_dataloader)) + ' Validation Loss: ' + str(validation_loss))\n",
        "\n",
        "  \n",
        "\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6901163166728581\n",
            "Epoch: 1/50 Training Loss: 0.7658413174152374 Validation Loss: 5.525486824159947\n",
            "0.6961393723905571\n",
            "Epoch: 2/50 Training Loss: 0.6406563619375228 Validation Loss: 5.274774885667301\n",
            "0.7029443882112819\n",
            "Epoch: 3/50 Training Loss: 0.6237607787847519 Validation Loss: 5.226679959747042\n",
            "0.6961541502994107\n",
            "Epoch: 4/50 Training Loss: 0.6127710593938828 Validation Loss: 5.4767363150233175\n",
            "0.6965970787518068\n",
            "Epoch: 5/50 Training Loss: 0.6083814537525177 Validation Loss: 5.36236419327047\n",
            "0.700936576284288\n",
            "Epoch: 6/50 Training Loss: 0.6034710947275161 Validation Loss: 5.462709636979239\n",
            "0.6893195940375518\n",
            "Epoch: 7/50 Training Loss: 0.602828196644783 Validation Loss: 5.623967661468575\n",
            "0.6927511530364829\n",
            "Epoch: 8/50 Training Loss: 0.5997678434848785 Validation Loss: 5.319826366201879\n",
            "0.694639157296395\n",
            "Epoch: 9/50 Training Loss: 0.5987647057771682 Validation Loss: 5.301644878029933\n",
            "0.6940373742032948\n",
            "Epoch: 10/50 Training Loss: 0.5970548651218415 Validation Loss: 5.285565727336307\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-09015abf5bc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m   \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    142\u001b[0m                    \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                    eps=group['eps'])\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = []\n",
        "\n",
        "for i in range(validation_vector.size()[0]):\n",
        "  prediction = model.forward(validation_vector[i])\n",
        "  predictions.append(torch.argmax(prediction).item())\n",
        "\n",
        "predictions\n",
        "y_validation.tolist()\n",
        "\n",
        "precision = precision_score(y_validation.tolist(), predictions, average='weighted')\n",
        "\n",
        "predictions\n",
        "y_validation.tolist()"
      ],
      "metadata": {
        "id": "Emp_YS-5hyqe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}