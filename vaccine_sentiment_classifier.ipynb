{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOaxr3FUS1BAEw0T1E171uM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/d-maniatakos/vaccine-sentiment-classifier/blob/master/vaccine_sentiment_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjnNdnolRJPx"
      },
      "source": [
        "## Module Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbn_yOIzRIaQ",
        "outputId": "51654d1c-c845-48ff-a37d-884af6f76c1c"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS"
      ],
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiWFJOfXUk0Z"
      },
      "source": [
        "## Data Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THqlw-35VXze"
      },
      "source": [
        "# read datasets and ignore the first (index) column\n",
        "train_data =  pd.read_csv('vaccine_train_set.csv').iloc[:, 1:]\n",
        "validation_data = pd.read_csv('vaccine_validation_set.csv').iloc[:, 1:]"
      ],
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IrDql26jJJf"
      },
      "source": [
        "## Study Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIQaHwF-jQii",
        "outputId": "befda0ac-07e9-4bcb-a6f4-d584402d11c7"
      },
      "source": [
        "train_data['label'].value_counts()"
      ],
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    7458\n",
              "2    6445\n",
              "1    2073\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 201
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSx1Gq_3jt50"
      },
      "source": [
        "It's clear that most tweets in the training dataset express a neutral opinion on vaccines, a little less tweets express a positive opinion and the least tweets express a negative ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuEuybY2sbyH"
      },
      "source": [
        "neutral_tweets = ' '.join(train_data[train_data.label== 1]['tweet'].tolist())\n",
        "negative_tweets = ' '.join(train_data[train_data.label== 1]['tweet'].tolist())\n",
        "positive_tweets = ' '.join(train_data[train_data.label== 2]['tweet'].tolist())"
      ],
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF7o4bAI1un3"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WqnUeLr1y9Z"
      },
      "source": [
        "# def preprocess(text):\n",
        "#   text = text.replace('@', '')\n",
        "#   text = text.replace('#', '')\n",
        "#   lemmatizer = WordNetLemmatizer()\n",
        "#   lemmatized_text = ''\n",
        "#   for token in text.split():\n",
        "#     lemmatized_text += lemmatizer.lemmatize(token) + ' '\n",
        "\n",
        "#   return lemmatized_text\n",
        "\n",
        "# train_data['tweet'] = train_data['tweet'].apply(preprocess)\n",
        "# validation_data['tweet'] = validation_data['tweet'].apply(preprocess)"
      ],
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p73tG5poWXr-"
      },
      "source": [
        "## Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ms1mq9JcWemk"
      },
      "source": [
        "def create_vectorizer(train_corpus, method='tf-idf', max_features=1000, ngram_range=(1, 2)):\n",
        "  if method == 'tf-idf':\n",
        "    vectorizer = TfidfVectorizer(max_features=max_features, ngram_range=ngram_range)\n",
        "  elif method == 'bow':\n",
        "    vectorizer = CountVectorizer(max_features=max_features, ngram_range=ngram_range)\n",
        "  return vectorizer.fit(train_corpus)\n",
        "\n",
        "def vectorize(vectorizer, corpus):\n",
        "  return vectorizer.transform(corpus)"
      ],
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WPvPpHAZWeh"
      },
      "source": [
        "## Model Creation & Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JKdAQSpZb5r",
        "outputId": "cc35d475-18b2-4429-ca4b-ab53120ba87c"
      },
      "source": [
        "train_corpus = train_data['tweet'].tolist()\n",
        "validation_corpus = validation_data['tweet'].tolist()\n",
        "\n",
        "\n",
        "vectorizer = create_vectorizer(train_corpus)\n",
        "train_vector = vectorize(vectorizer, train_corpus)\n",
        "validation_vector = vectorize(vectorizer, validation_corpus)\n",
        "\n",
        "train_features = pd.DataFrame(data=train_vector.toarray(), columns = vectorizer.get_feature_names_out())\n",
        "train_labels = train_data['label'].values\n",
        "\n",
        "classifier = LogisticRegression(multi_class=\"multinomial\")\n",
        "classifier.fit(train_features.to_numpy(), train_labels)\n",
        "\n",
        "predicted_labels = classifier.predict(validation_vector.toarray())\n",
        "precision = precision_score(validation_data['label'].tolist(), predicted_labels, average='weighted')\n",
        "recall = recall_score(validation_data['label'].tolist(), predicted_labels, average='weighted')\n",
        "f1 = f1_score(validation_data['label'].tolist(), predicted_labels, average='weighted')\n"
      ],
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5IjWdx3gn9M",
        "outputId": "a2b21b6e-91b6-4682-8d5a-22b021110aa0"
      },
      "source": [
        "print('Precision: ' + str(precision))\n",
        "print('Recall: ' + str(recall))\n",
        "print('F1-Score: ' + str(f1))"
      ],
      "execution_count": 236,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.5976358182752302\n",
            "Recall: 0.5438212094653813\n",
            "F1-Score: 0.47762350485514354\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52GKVqG87dHg",
        "outputId": "4e3e30e2-1117-4e00-8969-067b06a3089c"
      },
      "source": [
        "my_text = \"go vaccinate\"\n",
        "my_vector = vectorize(vectorizer, [my_text])\n",
        "\n",
        "classifier.predict(my_vector)\n",
        "\n"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2])"
            ]
          },
          "metadata": {},
          "execution_count": 219
        }
      ]
    }
  ]
}